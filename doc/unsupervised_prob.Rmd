---
documentclass: report
title: "Evaluating Unsupervised Probabilistic Models of the Microbiome"
date: "`r Sys.Date()`"
keep-tex: true
output:
  bookdown::pdf_book:
    keep_tex: yes
    includes:
      in_header: preamble.tex
bibliography: "unsupervised_prob.bib"
link-citations: yes
---

\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r knitr}
library("knitr")
chunk_opts <- list(
  fig.width = 5,
  fig.height = 3,
  fig.align = "center",
  cache = TRUE,
  warning = FALSE,
  message = FALSE,
  eval = FALSE
)

opts_chunk$set(chunk_opts)
```
\endgroup

## Context ##

Generally, microbiome studies attempt to characterize variation in microbial
abundance profiles across different experimental conditions
[@human2012structure] For example, it can be scientifically meaningful to
describe differences in microbial communities between treatment and control
groups. Alternatively, a study may attempt to describe changes in microbial
abundances after deliberately induced perturbations
[@dethlefsen2011incomplete].

In the process, there tend to be two complementary difficulties. First, the data
are often high dimensional, measured over several hundreds or thousands of
microbes. Studying patterns at the level of individual microbes is typically
infeasible. Second, it can be important to study microbial abundances in context
of known biological information. For example, if a collection of evolutionarily
related microbes all change in sync with each other, the scientific conclusion
would be different compared to if those microbes were not related to one
another. Further, metagenomic or proteomic data can often supplement raw
microbial abundance data, providing functional context.

### Data Description ###

In this study, we focus on 16S sequencing data, one of the original approaches
to quantifying microbial abundances, and still one of the most common protocols
for describing microbial ecology. Briefly, this procedure relies on the fact
that the 16S gene is conserved across most bacterial species, but is variable
enough to differ across different strains. Hence, by counting number of
occurrences of different versions of this gene, it is possible to measure the
numbers of different species present in a sample, without attempting the
difficult problem of assembling genomes from the mixtures present in complex
environmental samples.

After preprocessing, the resulting data are a samples by species counts matrix;
this is the fundamental object for exploration and inference. Some frequently
encountered characteristics of these type of data are worth noting. First, the
data are often very heavy tailed, with samples dominated by a relatively small
number of very abundant microbes. To address this, it is common to transform the
data, using variance stabilizing transformations or replacing with ranks, for
example, or dividing analysis between a "core" central component and a
peripheral collection of less abundant species. Second, these data are typically
zero-inflated, which is often accounted for by filtering\footnote{Using variance or
k-over-A filtering, for example.}. Third, at the sample level, there is
typically variation in sequencing depth. Hence, there is often better precision
in the estimates for some samples than for others. This difference is often
explicitly modeled [@mcmurdie2014waste; @love2014differential; @ren2016bayesian]

### Statistical Background ###

We now review a few of the statistical modeling techniques that we believe can
be useful building blocks when performing microbiome analysis. Many of these
techniques have been borrowed from text analysis, thinking of the samples by
microbes matrix as a biological analog of the usual document-term matrix. The
idea of transferring these techniques to the microbiome is not new, but the
methods have still yet to be widely adopted [@shafiei2015biomico;
@chen2012estimating; @holmes2012dirichlet;  @chen2013variable].

We include this review because we hope for this text to be self-contained.
Further, we expect these methods to only be the starting point for more
sophisticated analysis -- in fact, the extensibility and modularity of these
methods are some of their most attractive features.

#### Latent Dirichlet Allocation ####

Latent Dirichlet Allocation (LDA) is a generalization of multinomial mixture
modeling applicable to count matrices. We adopt the usual topic modeling
terminology, where each document is summarized by a vector of term counts.
Suppose there are $D$ documents across $V$ terms, and that these documents are
assumed mixtures of $K$ underlying topics, where a topic is defined as a
distribution over words. Then, LDA supposes the generative mechanism,
\begin{itemize}
\item For each word in the current document, draw a topic associated with that
word, with proportions determined by the document's fixed topic-mixture
probabilities.
\item Given the topics for each word in the document, draw a term associated
with it, according to the distribution for the topic to which that word is
assigned.
\end{itemize}

The specification is completed by defining priors for the document-topic and
topic-term distributions. We can formalize this process as follows. Let
$\theta_{d} \in \simplex^{K}$ represent the $d^{th}$ topic's mixture over the
$K$ underlying topics. Represent the term in the $n_{th}$ word of this document
by $w_{dn}$, and the associated topic by $z_{dn}$. Suppose the $k^{th}$ topic
places weight $\beta_{vk}$ on the $v^{th}$ term, so that $\beta_{\cdot k} \in
\simplex^{V}$. Then, the generative mechanism is

\begin{align*}
w_{dn} \vert \left(\beta_{\cdot k}\right)_{k = 1}^{K}, z_{dn} &\sim \Cat\left(\beta_{\cdot z_{dn}}\right) \\
z_{dn} \vert \theta_{d} &\sim \Cat\left(\theta_{d}\right) \\
\theta_{d} &\sim \Dir\left(\alpha\right) \\
\beta_{\cdot k} &\sim \Dir\left(\gamma\right).
\end{align*}
The interpretation is that each word in the document is assigned a topic
$z_{dn}$ according to the mixture weights $\theta_{i}$, and the actual terms are
only drawn afterwards, using the topic distributions $\beta_{\cdot k}$. In a standard
multinomial mixture model, each document can only belong to one topic, so there
would only be one value for $z_{dn}$ across all words.

In the microbiome application, we will find a formulation that marginalizes over
the $z_{dn}$ more convenient. Indeed, a strict analogy between text modeling and
microbiome analysis would consider each living microbe a word $w_{dn}$, while
what we are more interested in are counts of species across samples. That is,
we care more about $n_{dv} = \sum_{n = 1}^{N_{d}} \indic{w_{dn} = v}$ for each
sample $d$ and microbe $v$. Now, $n_{d\cdot} = \begin{pmatrix} n_{d1} \\ \vdots \\
n_{dV} \end{pmatrix} \in \naturals^{V}$ is a sum of $\Cat\left(\beta \theta_{d}
\right)$ variables, because $\Parg{w_{dn} = v} = \sum_{k = 1}^{K} \Parg{w_{dn} =
v \vert z_{dn} = k}\Parg{z_{dn} = k} = \beta_{v\cdot}^{T}\theta_{d}$. Hence, we
can write the marginal distribution as

\begin{align*}
n_{d\cdot} \vert \left(\beta_{k}\right)_{1}^{K} &\sim \Mult\left(n_{d\ast}, \beta \theta_{d}\right) \\
\beta_{\cdot k} &\sim \Dir\left(\gamma\right), k = 1, \dots, K \\
\theta_{d} &\sim \Dir\left(\alpha\right), d = 1, \dots D
\end{align*}
where $n_{d\ast} := \sum_{v = 1}^{V} n_{dv}$. Interpreting this model, we
imagine we think of each document $d$ as a mixture $\theta_{d}$ across $K$
topics.

The underlying model can be interpreted geometrically by identifying each topic
$\beta_{\cdot k}$ with a point on the $V$-dimensional simplex. Then the
expectation of the distribution over terms for the $d^{th}$ document, $\beta
\theta_{d}$ is a new point between the different $\beta_{\cdot k}$, with more
attraction towards those $\beta_{\cdot k}$ where $\theta_{dk}$ are large. See
Figure \@ref(fig:simplex_lda). That is, a collection of samples are modeled as
points lying anywhere between the $K$ latent topics. This is in contrast to
multinomial mixture modeling, which identifies each sample with one of the $K$
points $\beta_{\cdot k}$, but similar in spirit to Correspondence Analysis,
which also tries to focus interest on subsets of $\simplex^{V}$
[@greenacre1987geometric].

\begin{figure}
\centering
\includegraphics[width=7.5cm]{figure/lda_simplex_view}
\caption{The simplex view of Latent Dirichlet Allocation. Figure from [@blei2003latent].}
\label{fig:simplex_lda}
\end{figure}

### Dynamic Unigrams ###

LDA can be adapted to incorporate more specific structure. In light of the
longitudinal nature of many modern microbiome studies, one of the most
intriguing of these variants is called the Dynamic Topic Moderl
[@blei2006dynamic; @wang2012continuous]. This is discussed in section
\@ref(sec:dtm) -- here, we describe a related, simpler model, called the Dynamic
Unigram model, which was also investigated in [@blei2006dynamic].

The basic idea in the unigram model is to adapt the geometric view of LDA to the
time-series setting. In more detail, while LDA imagines samples being mixtures
of fundamental topics on the $V$-dimensional simplex, the Dynamic Unigrams Model
identifies a sequence of samples over time with a curve along this simplex. That
is, suppose $n_{d \cdot}$ is a document at time $t\left(d\right)$. Then, we
imagine $n_{d \cdot} \sim \Mult\left(\beta_{t\left(d\right)}\right)$, where $\beta_{t}$
are topic probabilities smoothly evolving along the simplex\footnote{We are
overloading notation in writing $t$ both as a time index for the underlying
probabilities $\beta_{t}$ and as a mapping from a document $d$ to the time that it
was observed, $t\left(d\right)$}.

A complication arises that it is not obvious how to define a prior on $\beta_{t}$
so that they trace out a curve on the simplex -- it is no longer enough to set
$\beta_{t} \sim \Dir\left(\alpha\right)$. An alternative is to link a gaussian
random walk to the simplex using a logit-normal prior. That is, define

\begin{align*}
n_{d\cdot} \vert \mu_{t\left(d\right)}  &\sim \Mult\left(\sum_{v} n_{dv}, S\left(\mu_{t\left(d\right)}\right)\right) \\
\mu_{t} \vert \mu_{t - 1} &\sim \Gsn\left(\mu_{t - 1}, \sigma^{2}I_{V}\right) \\
\mu_{0} &\sim \Gsn\left(0, \sigma^{2}I_{v}\right).
\end{align*}
where $S$ is the multilogit link
\begin{align*}
\left[S\left(\mu\right)\right]_{v} = \frac{\exp{mu_{v}}}{\sum_{v^{\prime}} \exp{\mu_{v^{\prime}}}}.
\end{align*}

This is just a reparameterization, allowing us to work with
$S\left(\mu_{t}\right)$ instead of directly with $\beta_{t}$. Note that the only
tuning parameter here is $\sigma^{2}$, which governs the smoothness of
transitions in term distributions over time. There is no particular reason to
choose a random walk instead of any more general ARMA process for the prior
$\left(\mu_{t}\right)$, except simplicity of presentation.

The estimated $\hat{\beta}_{tv}$ can be thought of as a smoothing of the raw
proportions $\hat{p}_{tv}:= \frac{\sum_{d : t\left(d\right) =
t}n_{dv}}{\sum_{d : t\left(d\right) = t}\sum_{v^{\prime}} n_{dv^{\prime}}}$.
That is, while LDA can be thought of a modification of latent factor modeling to
counts, the dynamic unigrams model can be thought of as the corresponding
variant of nonparametric smoothing.

### Dynamic Topic Models {#sec:dtm}

Unlike LDA, there is no notion of underlying topics in the Dynamic Unigrams
Model, but blending the two modeling ideas leads straightforwardsly to the
Dynamic Topic Model [@blei2006dynamic, @wang2012continuous]. Here, like LDA,
documents are modeled as mixtures latent topics. However, both the topics and
the mixture probabilities are modeled to evolve gradually over time.
Geometrically, the points $\beta_{\cdot k}$ within which all documents lie are
thought to shift over time. Further, the document mixtures $\theta_{d}$ aren't
scattered in the simplex without regard to time. Instead, the mixing
probabilities $\theta_{d}$ are also thought to exhibit some inertia, being more
similar among documents closer to each other in time.

This is formulated more precisely as 
\begin{align*}
n_{d\cdot} \vert \mu_{t\left(d\right)}, \alpha_{\left(d\right)} &\sim \Mult\left(\sum_{v} n_{dv}, \sum_{k = 1}^{K} S\left(\alpha_{t\left(d\right)}\right) S\left(\mu_{t\left(d\right)}\right)\right) \\
\alpha_{t} \vert \alpha_{t - 1} &\sim \Gsn\left(\alpha_{t - 1}, \delta^{2}I_{K}\right) \\
\mu_{t} \vert \mu_{t - 1} &\sim \Gsn\left(\mu_{t - 1}, \sigma^{2}I_{v}\right) \\
\alpha_{0} &\sim \Gsn\left(0, \delta^{2}I_{K}\right) \\
\mu_{0} &\sim \Gsn\left(0, \sigma^{2}I_{K}\right)
\end{align*}

Here, we think of $S\left(\mu_{tk}\right)$ and $S\left(\alpha_{t}\right)$ as
parameterizing $\beta_{tk}$ and $\theta_{t}$, respectively. Here, $\delta^{2}$
and $\sigma^{2}$ govern the rate at which mixture probabilities and topics can
evolve. A smaller $\frac{\delta}{\sigma}$ ratio bring us closer to dynamic
unigrams, because the documents mixing proportions are closer to being fixed
while the topics evolve. Conversely, large ratios $\frac{\delta}{\sigma}$ bring
us closer to LDA, with samples mixing across more or less fixed topics.

### Nonnegative Matrix Factorization ###

Consider the samples by microbe matrix, $N \in \reals^{D \times V}$ made from
counts $n_{dv}$. In LDA, these counts are modeled by sampling from a
multinomials with total counts coming from the total number of words in each
document and probabilities coming from the rows of $\Theta B^{T}$ where $\Theta
= \begin{pmatrix}\theta_{1} \\ \vdots \\ \theta_{D} \end{pmatrix} \in
\left(\simplex^{K - 1}\right)_{\downarrow \times D}$ and $B = \begin{pmatrix}
\beta_{\cdot 1} \dots \beta_{\cdot K} \end{pmatrix} \in \left(\simplex^{V -
1}\right)_{\rightarrow \times K}$ are $D \times K$ and $V \times K$ matrices
representing document and topic distributions, respectively.

This has the flavor of factor analysis with $K$ factors, but endowed with
specific multinomial structure. To generalize without devolving into ordinary
factor analysis, which doesn't respect any of the count structure in the data,
it is natural to consider nonnegative matrix factorization, which approximates
the nonnegative matrix $N$ by the product of low rank matrices, $N \approx
\Theta B^{T}$, where now the only constraints on $\Theta$ and $B$ are that
$\Theta \in \reals_{+}^{D \times K}$ and $B \in \reals_{+}^{V \times K}$. This
is the starting point for a variety of algorithms in the Nonnegative Matrix
Factorization (NMF) literature [@wang2013nonnegative; @berry2007algorithms,
@lee2001algorithms].

Here, we will consider a Gamma-Poisson factorization model (GaP)
[@kucukelbir2015automatic; @canny2004gap], which is proposes the hierarchical
model
\begin{align*}
N &\sim \Poi\left(\Theta B^{T}\right) \\
\Theta &\sim \Gam\left(a_{0} 1_{D \times K}, b_{0} 1_{D \times K}\right) \\
B &\sim \Gam\left(c_{0} 1_{V \times K}, d_{0} 1_{V \times K} \right),
\end{align*}
where mean that each entry in these matrices is sampled independently, with
parameters given by the corresponding entry in the parameter matrix.

Unlike LDA, GaP models the total number of appearances of each topic implicitly
via the gamma prior on $\Theta$. When the shape parameter is small, this prior
is quite skewed -- this reflects the suspicion that most samples will be
dominated by only a few of the $\beta_{\cdot k}$. Further, when the shape
parameter is large, the gamma converges to a normal distribution, and so the GaP
model then reduces to ordinary factor analysis. Finally, the gamma prior induces
overdispersion in draws from the model, paralleling the usual interpretation of
the negative binomial distribution as the result of marginalizing a gamma prior
in a Poisson model.

We can introduce an extra layer into this model to reflect zero-inflation, which
is a common feature of many microbiome data sets [@xu2015assessment;
@mcmurdie2014waste]. The change in the model is minor -- with some probability
$p$ entries in $Y$ are sent to zero. That is, instead of of the Poisson
likelihood,
\begin{align*}
\Parg{Y_{ij} = y_{ij} \vert \theta_{i}, \beta_{j \cdot}} = \frac{\exp{-\theta_{i}^{T}\beta_{j \cdot}} \left(\theta_{i}^{T}\beta_{j}\right)^{y_{ij}}}{y_{ij}!}, 
\end{align*}
we have
\begin{align*}
\Parg{Y_{ij} = y_{ij} \vert p, \theta_{i}, \beta_{j \cdot}} = \begin{cases}
p + \exp{-\theta_{i}^{T}\beta_{j\cdot}} &\text{ for } y_{ij} = 0 \\
\left(1 - p\right)
\frac{\exp{-\theta_{i}^{T}\beta_{j
\cdot}}\left(\theta_{i}^{T}\beta_{j}\right)^{y_{ij}}}{y_{ij}!} &\text{ for }
y_{ij} > 0.
\end{cases}
\end{align*}

### Probabilistic Programming ###

Probabilistic programming tools provide generic interfaces to constructing
Bayesian samplers [@plummer2003jags, @carpenter2016stan,
@kucukelbir2015automatic]. These tools simplify the process of specifying
complex likelihoods by providing an interface to a set of fundamental sampling
statements. Once the model is described, standard MCMC can be
applied\footnote{This step usually means only models with continuous parameter
spaces can be implemented, since Bayesian inference with discrete parameters is
less amenable to general solutions.}

For example, the NMF model can be implemented using essentially only the STAN
code below\footnote{The complete code is available at
\url{https://github.com/krisrs1128/readings/blob/master/nmf/src/nmf_gamma_poisson.stan}}.

```python
model{
  for(i in 1:D) {
    theta[i] ~ gamma(a,b); // componentwise gamma
  }

  for (j in 1:V) {
    beta[j] ~ gamma(c,d);//componentwise gamma
  }

  for (i in 1:D) {
    for (j in 1:V) {
      y[i, j] ~ poisson(theta[i]' * beta[j]);
    }
  }
}
```
The sampling statements `~ gamma` and `~ poisson` tell STAN to increment the
loglikelihood used to construct samplers.

Access to this interface for doing Bayesian inference simplifies the process of
exploring alternative probabilistic models. Indeed, implementing samplers ``by-hand'' 
can be a time-consuming and error-prone process. All simulation and data
analysis results described in this report have been implemented in STAN and are
available at (todo: central repo).

## Simulation Studies ##

It can be liberating to have easy access to such a variety of modeling
strategies for any given data analysis problem. However, with this increased
flexibility comes the difficulty of knowing which methods to use when. Perhaps
more unsettlingly, it is sometimes unclear whether or not the results of a model
can be trusted at all -- there may be issues about parametric assumptions or
sample size, for example.

To build some intuition about estimation accuracy across combinations of data
settings and model types, we conduct some simulation studies. These are meant to
complement the model-checking that should follow paramtric analysis -- since we
know the truth in simulations, it is easier to develop more definitive
guidelines.

The difficulty wth simulations, of course, is that it is hard to for them to
reflect the diversity of data analysis settings that arise in practice. We
choose a few situations we find interesting, but there are many other situations
worth investigating.

### NMF {#sec:nmf_sim} ###

\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r read-nmf}
read_chunk("../src/sim/nmf/nmf_expers.R")
```
\endgroup

\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r libraries-nmf-expers}
```
\endgroup

We specify our experiment using a JSON file whose elements provide simulation
and model fitting parameters. We fix the total number of samples at 100 and
latent factors at 2. We set the dimension to either 75 or 125. We alternate
fitting using Gibbs sampling and Variational Bayes. We vary the zero inflation
probability across $\{0, 0.2\}$, and fit using either the zero-inflation
likelihood with known zero inflation probability or using the GaP model which is
unaware of zero-inflation. In all, there are 16 configurations. These simulation
parameters are set up in the configuration \texttt{data.frame} below, which is
output to \texttt{config.json} so that it can be read by the batch script, which
is used to parallelize the simulation.

\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r configuration}
```
\endgroup

Now that the simulation settings have been set, we send them off to our computer
topic, and wait for the results to get written. The code below creates the
batch scripts that run subsets of the experimental configurations.

\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r submit-jobs }
```
\endgroup

Once the experiments complete, we can visually assess the quality of the fits.
First, we read in the fitted parameters across experiments and merge them into a
single object, suitable for plotting. 

\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r read-vis}
read_chunk("../src/sim/nmf/nmf_vis.R")
```
\endgroup

\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r libraries-nmf-vis}
```
\endgroup

\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r theta-reshape}
```
\endgroup

Figures \@ref(fig:visualizethetas) and \@ref(fig:visualizebetas) display raw
experimental results obtained when working with the non-zero-inflated data. In
Figure \@ref(Fig:visualizethetas), each panel represents the underlying sample
scores $\theta_{i}$. Black labels give the true positions of the latent scores
for the associated sample, while red labels give the average across all
posterior samples for that data point. The blue-green clouds are the contours
along posterior samples -- they give a sense of the variability of the estimated
scores. Note that the $x$ and $y$-axes are displayed on a square-root scale.

In the grid of plots, the columns vary over underlying data dimension $V$ while
rows vary over fitting algorithm (Gibbs Sampling and Variational Bayes) as well
as number of samples $D$. In particular, the true scores (black numbers) are in
the same positiion for every unique $\left(D, V\right)$ combination.

A few general patterns are suggested. First, the approximate posteriors from
Variational Bayes seems less spread out than those from Gibbs sampling. In
particular, the samples tend to avoid $\theta_{i}$'s with very uneven values
between the two coordinates. This effect is especially prominent in the
low-dimensional setting, when there is less data available for inference of each
individual $\theta_{i}$, and where there are quite a few true $\theta_{i}$'s
that fail to fall into any of the posterior clouds.

Second, we note that the high-dimensional regime does have somewhat more
concentrated posteriors, as indicated by the brighter green shade. This effect
is small but consistent, especially in Gibbs Sampling.

\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r visualizethetas-prep}
```
\endgroup

\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r visualizethetas, fig.height = 4, fig.cap = "Visualized scores from the NMF experiment, on data without any zero inflation."}
```
\endgroup

While useful in gauging the global properties of NMF inference across regimes,
it is difficult to assess the accuracy of the estimates by comparing red and
black labels. An alternative view compares the true latent score to each sampled
from the posterior. A histogram of these differences is displayed in Figure
\@ref(fig:visthetashist). Each panel has the same meaning as before. Each
histogram are of errors $\hat{\theta}_{ik} - \theta_{ik}$; orange is for $k = 1$
and purple is for $k = 2$, though the error distributions for the two factor
dimensions are quite similar and so mix into a burgundy. The fact that each
sample $i$ has its own error distribution is visible in the way the histograms
look like mixtures.

Here, the improved concentration in the high-dimensional ($V = 125$) regime is
more readily apparent. Further, the superiority of gibbs sampling in the
lower-dimensional ($V = 75$) case is still noticeable in the fact that the gibbs
histogram is somewhat narrower in the center. However, while there seems to be
less mass in the tails for the Gibbs histogram, the tails themselves are just as
wide as in the Variational Bayes histogram. Further, the fact that these
histograms are not centered around zero reflects bias in the estimators -- they
seem to be systematically too large.

\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r visualizethetashist, fig.cap = "Histograms of the differences between posterior sampled scores and their true values."}
```
\endgroup

Our approach to analyzing the posteriors on $\beta_{\cdot k}$ parallels that for
studying the $\theta_{i}$. Figure \@ref(fig:visbeta) is interpreted similarly to
Figure \@ref(fig:visthtea) before, but now each point is coordinate
$\beta_{v\cdot}$. Note that when $V$ increases, the number of such points
increases.

\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r visualizebetas, fig.height = 4, fig.cap = "Posteriors for the two-dimensional latent factors, across simulation settings."}
```
\endgroup

Again, Variational Bayes seems to underestimate the posterior variances of each
estimate and generally avoids placing mass on $\beta_{v\cdot}$s near the axes.
This seems to be true in both lower and higher-dimensional regimes, though the
fact that the number of points changes across columns makes them difficult to
compare. The Gibbs sampling estimates seem more accurately, generally, but we
transition to the error histogram view to establish this more firmly.

Figure \@ref(fig:visbetahist) is the analog of \@ref(fig:visthetahist) for the
$\beta_{v\cdot}$s. The posterior seems more concetrated around the truth in the
high-dimensional case, in spite of the fact that each $\beta_{v\cdot}$ has
access to the same number of samples across regimes.

\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r visualizebetashist, fig.cap = "Histograms of the differences between posterior sampled factors and their true values."}
```
\endgroup

### Zero-inflated NMF ###

Now we examine the effect of zero inflation on estimation of the $\theta_{i}$
and $\beta_{v}$. Our interest revolves around two questions,

* In the case that zero-inflation is present and the appropriate model, with
  known zero inflation probability, is used, how quickly do estimates
  deterioriate as more and more entries are sent to zero? 
* What are the consequences for estimating latent scores and factors, in the
  case that zero-inflation exists but is not accounted for?

To this end, we focus on the data in the lower-dimensional regime ($P = 75$) and
compare variation in model estimates across zero-inflation probabilities and
fitting procedures. Figure \@ref(fig:visualizezinfthetas) displays the
posteriors for the $\theta_{i}$ across such configurations. As before, the two
rows of plots distinguish between inference by Gibbs sampling and Variational
Bayes. Columns indicate whether or not zero-inflation was accounted for in
modeling (Z-GaP vs. GaP) and the underlying zero inflation probability.

\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r visualize-zinf-thetas-prep}
```
\endgroup

\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r visualizezinfthetas, fig.height = 8, fig.width = 8, fig.cap = "Each panel displays the estimated scores for a different simulation setting, with a focus on the effect of zero inflation. Columns distinguish between different zero-inflation probabilities and modeling techniques."}
```
\endgroup

While the differences observed between Gibbs Sampling and Variational Bayes
continue to be clear, it is not obvious from these figures whether there is any
difference in estimation accuracy across columns. To study this more precisely,
we again consider the error histograms associated with each panel; this is given
in Figure \@ref(fig:visualizezinfthetashist).

The most noticeable difference across these panels is that Variational Bayes
seems to perform substantially worse when zero-inflation is present, especially
when zero-inflation is not taken into account. Surprisingly, Gibbs Sampling
seems only slightly worse in this situation. Even more oddly, the case in which
zero-inflation is not explicitly modeled does not seem to do worse than the case
it is.

\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r visualizezinfthetashist, fig.cap = "For each simulation setting, we can compute the true estimation errors across samples."}
```
\endgroup

Figures \@ref(fig:visualizezinfbetas) and \@ref(fig:visualizezinfbetashist) are
the corresponding views of the latent factors $\beta_{v\cdot}$. Both figures
reinforce the findings from those based on the estimated scores $\theta_{i}$,
namely, Variational Bayes deterioriates more rapidly upon introduction
zero-inflation, and Gibbs Sampling results seem comparable under both GaP or
Z-GaP models.

\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r visualizezinfbetas, fig.height = 6, fig.width = 6, fig.cap = "The analogous display for estimated latent factors, across zero-inflation regimes."}
```
\endgroup

\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r visualizezinfbetashist, fig.cap = "Across settings, we can explicitly compute estimation errors."}
```
\endgroup

### LDA ###

Here, we consider an analogous experiment to evaluate inference in the LDA
model. We again consider several data generation regimes, varying

* $D$: The number of samples.
* $V$: The number of features.
* $N$: The total count within each sample.

We do not vary the total number of topics or the Dirichlet hyperparameter
strength. Further, we always assume the model is corretly specified before doing
inference. Nonetheless, even in this relatively restricted setting, we find
certain observations worth sharing.

In addition to comparing MCMC sampling with VB inference, we consider a
parametric bootstrap approach to generating confidence sets. In this approach,
we perform an initial VB fit to obtain posterior mean $\hat{\theta}$ and
$\hat{\beta}$. Then, $B$ samples are simulated from an LDA model with these
parameters, and separate VB fits are computed for each of these. We consider
this approach in light of the suspicion (confirmed in the simulation) that VB
typically underestimates the variance of fits, but that MCMC sampling is harder
to parallelize than bootstrapping. The hope then would be that bootstrapping
offers the best of both world -- the speed of VB with the accuracy of MCMC. We
discuss whether our simulation study substantiats this idea below.

We take $D \in \{20, 100\}$, $V \in \{10, 50\}$ and $N \in \{20, 50, 100\}$ and
then simulate under all 12 combinations of these three parameters. Throughout,
we fix $\alpha_{0} = \gamma_{0} = 1$ and $K = 2$. With these parameters, we aim
to characterize performance of different inferential techniques under different
signal-to-noise regimes, but avoid any investigation of robustness.

#### Implementation ####

This section describes ideas used to streamline the practical implementation of
the simulation study. The remaining sections, focusing more on statistical
consequences of the simulation, can be read independently. We believe that some
of the techniques used in this simulation could be useful in a more general
context, however, and so ar worth discussing.

The main source complexity in the simulation study are the dependencies between
different data generation and fitting procedures. For example, the bootstrapping
approach requires the initial VB fit for the given experimental configuration be
available, which itself requires that data from the associated LDA model is
already simulated. In addition to this dependence between simulation tasks, it
is desirable to parallelize as much as possible. For example, different
experimental configurations can be run independently. There are also
intermediate cases, where some parts of the pipeline can be parallelized, but
the rest cannot be -- consider simulating data from the same underlying
$\theta_{i}$ and $\beta_{k}$, but with different numbers of words per document
$N$.

In data engineering, it is common to think of data processing and modeling
pipelines with nontrivial structure as directed acyclic graphs. We adopt this
point of view in our simulations, encoding the task depednence structure using
the Luigi library [@leipzig2016review], which then manages parallelization and
execution order of the necessary simulation tasks. The full graph is displayed
in Figure \ref{fig:luigi_pipeline}.

A further advantage of this framework is that new experiments can make use of
simulation output from earlier ones. For example, new models can be run on
previously generated data sets, without recreating them\footnote{By setting
seeds at every potential source of randomness, resimulating data would not lead
to any change in the resulting data set. The downside of this approach is lack
of computational efficiency, not experimental validity.}

#### Topics $\beta_{\cdot k}$ ####

The code below retrieves the output from the simulation experiments and prepares
them for visualization.

\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r read-boot-expers}
read_chunk("../src/sim/lda/vis_exper.R")
```
\endgroup

\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r libraries-boot-expers}
```
\endgroup

\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r paths}
```
\endgroup

##### Posterior Scatterplots #####

The results of this simulation are summarized in Figures \@ref(betacontours1)
through \@ref(betahistograms). Note that topics have been heuristically aligned
across all samples, to avoid the label-switching problem\footnote{Specifically,
we take the correlation between fitted and true topic vectors, and first
identify the fitted and true topic with highest correlation. Among the remaining
topics, we align those with the next highest correlation, and so on.}.

\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r beta-samples}
```
\endgroup

\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r beta-alignment}
```
\endgroup

Figure \@ref(betacountours1) displays the true and posterior $\beta_{k}$ for the
experiments with $V = 10$ features, while other simulation characteristics are
varied. As before, each panel represents a single experimental configuration,
with two axes associated with the two underlying topics. Each black number $v$
is the true value of feature $v$ across topics: $\left(\beta_{v1},
\beta_{v2}\right)$. The shaded clouds are sampled posteriors, while the orange
labels give posterior means.

Across rows, different inferential procedures are compared -- \texttt{bootstrap}
refers to the bootstrap-after-variational bayes scheme, \texttt{gibbs} is MCMC
sampling, and \texttt{vb} is variational bayes. Across columns we vary $D$, the
total number of samples, and $N$, the total count within each sample. In
particular, across the first and last three columns, the true $\beta_{vk}$
values remain unchanged. Figure \@ref(fig:betacontours2) is read in exactly the
same way.

\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r beta-contours-object}
```
\endgroup

\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r betacontours1, fig.width = 7, fig.height = 4, fig.cap = "True and fitted parameters $K = 2$ topic distributions across $V = 10$ features. Different columns describe different $\\left(D, N\\right)$ combinations, while rows give alternative fitting approaches."}
```
\endgroup

\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r betacontours2, fig.width = 7, fig.height = 4, fig.cap = "The analogous figure, with distributions over $V = 50$ instead of 10 features."}
```
\endgroup

A few comments are in order. As expected, when $D$ increases, i.e., more samples
are made available, the posterior for $\beta$, whose dimension do not increase
with $D$, begins to concentrate around its true value. It is unclear whether the
rate of concentration when $V = 50$ is much different from the case where $V =
10$, because the absolute sizes of the clouds in Figure \@ref(fig:betacontours2)
could result from their being more fitted parameter values being displayed in
the same space, though see \@ref(fig:betahistograms) for more clarity on this
point.

Consistent with earlier findings, the VB posteriors are generally lower variance
and more elliptical than the true MCMC sampled posteriors. The bootstrap samples
seem somewhere between the VB and MCMC sampled distributions in terms of
variablity within each feature. As the number of samples or number of words
within samples increases, the three methods become indistinguishable.

Interestingly, when $D$ and $N$ are small ($N = D =20$), VB appears more
accurate than either the MCMC or bootstrapping approaches, whose posterior means
all lie along the diagonal, corresponding to topics that both reflect the global
marginal feature counts across samples. This effect is especially pronounced in
the case $V = 50$. Here, even the $D = 20, N = 50$ configuration seems to lead
to result in inference issues for the bootstrap. We have no explanation for this
phenomenon.

##### Alternative Visualization #####

The scatterplots presented above are informative in the case that $K = 2$, but
for larger $K$, it becomes unwieldy to display all $K \choose 2$ pairs of
topics as scatterplots. We provide two alternative visualizations that are
better adapted to this situation: error histograms, like those in Section
\@ref(sec:nmf_sim), and faceted boxplots.

The error histograms are given in Figure \@ref(fig:betahistograms). In each
panel, there are two overlapping histograms -- one orange and one purple --
representing the errors, on a square-root scale, for the two topics. Rows
index different fitting methods and numbers of features $V$, while columns index
different $D$ (first row of labels) and $N$ (second row of labels).

\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r betahistograms, fig.height = 6, fig.cap = "Histograms of the difference between sampled topic parameters and their true values, across experimental configurations."}
```
\endgroup

Compared to the NMF simulation, the histograms look less strongly like mixture
distributions. Further, the tails seems somewhat lighter -- only MCMC with small
sample sizes has large tails.

As expected, for fixed $D$, as $N$ increases, the errors become more
concentrated around 0, and when $D$ is larger, this concentration is apparent
even for smaller $N$. All three methods seem equivalent in larger sample sizes.
When fewer samples are available, the error distributions are widest for MCMC,
though the bootstrap distributions are similarly shaped, only with lighter
tails. The VB distributions are more irregularly shaped, not necessarily
symmetric around zero, and with substantial lack of overlap between topic
dimensions.

An alternative view, which does not require looking only at errors, is given in
Figures \@ref(fig:betaboxplot1) and \@ref(fig:betaboxplot2), which are the
boxplot analogs of Figures \@ref(fig:betacontours1) and
@\ref(fig:betacontours2), respectively. Within each panel, the posteriors for a
single topic are represented by boxplots, with different colors corresponding to
different inferential methods. Further, the true value for that parameter is
given by a grey horizontal line.

\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r beta-boxplots-object}
```
\endgroup

\blandscape
```{r betaboxplot1, fig.width = 10, fig.height = 6, fig.cap = "Boxplots of posterior samples for the $\\beta_{\\cdot k}$ across features, inferential techniques, and experimental configurations, when $V = 10$.", echo = FALSE}
```
\elandscape

\blandscape
```{r betaboxplot2, fig.width = 10, fig.height = 6, fig.cap = "Boxplots of posterior samles, when $V = 50$.", echo = FALSE}
```
\elandscape
 
Different columns index different features -- in Figure \@ref(fig:betaboxplot1),
$V = 10$, while in Figure \@ref(fig:betaboxplot2), $V = 50$. Rows index
different topics and experimental configurations. From inside out, the $y$-axis
facet labels give $K$, $N$, and $D$, respectively. Note that the true values are
not the same across experimental configurations, due to the simulation setup.

The takeaways from these views of the simulation experiment are consistent with
all others so far. One pattern that is not obvious from the other displays is
that MCMC sampling can lead to posteriors with high skewness in the case that a
parameter is close to one of the endpoints of its domain (0 or 1, in this case). 

#### Mixing Proportions $\theta_{i}$ ####

For completeness, we can perform a similar analyis on the recovered mixing
proportions $\left(\theta_{i}\right)$. The block below combines output saved
from different experimental configurations, in order to simplify visualization
below.

\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r theta-samples}
```
\endgroup

\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r theta-alignment}
```
\endgroup

##### Posterior Boxplots #####

Across our simulation, we have fixed the number of topics $K = 2$. Since, for
each sample $i$, $\theta_{i1} + \theta_{i2} = 1$, scatterplots like those in
Figures \@ref(fig:betacontours1) and \@ref(fig:betacontours2) are not
appropriate -- there is only one degree of freedom. Instead, we first provide
the analogs of Figures \@ref(fig:betaboxplot1) and \@ref(fig:betaboxplot2);
these are in Figures \@ref(fig:thetaboxplot1) and \@ref(fig:thetaboxplot2).

\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r theta-boxplot-object}
```
\endgroup

\blandscape
\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r thetaboxplot1, fig.width = 10, fig.height = 6, echo = FALSE, fig.cap = "Estimated and true proportions $\\theta_{i1}$ across samples, for simulations where $D = 20$."}
```
\endgroup
\elandscape

\blandscape
\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r thetaboxplot2, fig.width = 10, fig.height = 6, echo = FALSE, fig.cap = "The analogous figure, when $D = 100$"}
```
\endgroup
\elandscape

Here, each panel gives the posterior for $\theta_{i1}$, within an experimental
configuration. The index $i$ is specified by the labels across columns. The
three columns of labels for panels along the $y$-axis give $N$, the total count
within each sample, and $V$, the number of measured features. Transparent
horizontal bars specify the true value $\theta_{i1}$ in each configuration.

Increases in sample size are reflected in the narrowing of boxplots and closer
agreement between methods. As before, MCMC sampling outputs posteriors
specifying higher uncertainty, and bootstrapping and MCMC tend to agree with
each other more closely in the small sample size situation.

Some patterns emerge in estimation of the $\theta_{i}$s that were not apparent
when estimating the $\beta_{\cdot k}$s. First, estimation can be very biased
for some coordinates, especially when $N$ is small, but even for many
coordinates when $N$ is large. Generally this bias generally tends to pull
coordinates towards the prior, making the estimated $\theta_{i1}$s flatter
across $i$s. Second, VB behaves quite differently from MCMC and the bootstrap
when $N$ is small, sometimes better (column 15 in row 1) and sometimes worse
(column 6 in row 1). We also notice that when $V$ is larger, the uncertainty in
estimated $\theta_{i}$ increases, perhaps because estimation of the underlying
topics deteriorates. In this sense, LDA is not transposable -- when estimating
$\beta_{vk}$, increasing $N$ reduced uncertainty, while for estimating
$\theta_{ik}$, increasing $V$ increases uncertainty.

We conjecture that these differences between estimating topics and mixing
proportions is a result of the fact that the $\theta_{i}$s are "local"
parameters, whose number grow with the sample size, while the $\beta_{\cdot k}$s
are "global" parameters, whose number are indepenent of sample size.

##### Error Histograms #####

To more directly assess the precision of estimates across methods and
experimental configurations, we again histogram the discrepancies between
samples and known simulation values. The increasing narrowness in histograms
moving from left to right in groups of three reflects increasing sample sizes.
The last three columns tend to have slightly wider histograms than the the first
three, corresponding to higher errors for $V = 50$ relative to $V = 10$.

The $V = 10$, $N = 100$, $D = 20$ case has error distributions for the two
topics that seem to be shifted relative to one another, though we have no
explanation why this effect only appears here and the $50, 50, 20$ VB case.

Comparing methods, for small $N$ and $D$, MCMC has the widest error histogram,
followed by VB and the bootstrapping. The width in the MCMC samples errors
reflects the larger uncertainty in the MCMC posteriors. It is possible that the
MCMC posterior mean might be more accurate, but we only have a single posterior
mean for each experimental configuration, so we cannot make this judgement.

\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r thetahistograms}
```
\endgroup

## Data Analysis Case Studies ##

In applying probabilistic methods to practical microbiome data analysis, two
questions immediately come to mind,

1. Do these models fit well to the raw or preprocessed data, and what techniques
are available for evaluating model fit?
2. Supposing these models fit well, do they lend themselves to informative
summaries of the original data?

To begin to develop answers to these questions, we reanalyze the data of
[@dethlefsen2011incomplete], a study of microbial dynamics in response to
antibiotic treatment. This study monitored the microbiomes of three patients
over ten months, with two antibiotics time courses introduced in between, in
order to study the effect antiobiotic perturbations within the context of
natural long term dynamics. The study concluded that antibiotics cause
substantial changes in short-term community composition, with certain species
being substantially more resilient than others, and that in one patient,
long-term effects could be observed. The purpose of this data analysis is to
evaluate whether similar conclusions are evident when applying the unsupervised
probabilistic methods described so far.

### Data Preparation ###

Before pursuing formal inference, we filter and transform the raw data, in line
with the preprocessing steps in standard microbiome analysis. The code below
sets up the environment for the rest of this data analysis application.

\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r read_lda}
read_chunk("../src/antibiotics-study/lda.R")
```
\endgroup

\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r setup}
```
\endgroup

In light of the fact that variational in microbial signatures tends to be
dominated by between individual effects, we choose to study one individual at a
time. In this report, we focus on Subject F, who had been reported to exhibit
incomplete recovery of the pre-antibiotic microbial community. Further, we
filter to only those microbes present in at least 45\% of samples. This reduces
the dimensionality from 2582 to 354.

\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r get-data}
```
\endgroup

We next transform counts so that they are not so heavily skewed -- otherwise,
samples tend to be dominated by a few very highly abundant microbes. Since there
are many zeros, but the counts are highly skewed, we apply an $\asinh$
transformation, which behaves like $\log$ for large values, but is defined at 0.
The difference between the histograms across raw and transformed data is
displayed in Figure \@ref(fig:histograms), and the corresponding heatmaps for
the original data matrices are given in Figure \@ref(fig:heatmaps).

\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r histograms}
```
\endgroup


\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
\begin{}
```{r histograms_figure, fig.cap = "Even after filtering, the data are very heavy tailed. Upon $\asinh$ transformation, the data are more regularly shaped, though still somewhat heavy tailed."}
include_graphics("figure/histograms-1.png")
```
\endgroup

\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r heatmaps}
```
\endgroup

\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r heatmaps_figure, fig.show = "hold", fig.cap = "Heatmaps of counts on the filtered data, before (left) and after (right) transformation, with microbes and samples sorted according to their average value."}
include_graphics("figure/heatmaps-1.png")
include_graphics("figure/heatmaps-2.png")
```
\endgroup

With these preprocessing steps out of the way, we can proceed with modeling.

### LDA {#sec:antibiotics-lda}

We first apply LDA directly. Note that, after the $\asinh$ transformation, the
data are no longer actually counts. To make the model applicable, we round to
the nearest integer -- the model here is really only a device, not a plausible
data generating mechanism. In particular, uncertainty estimates for fitted
parameters should be regarded skeptically.

\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r rounding}
```
\endgroup

#### Estimation ####

We fit LDA with $K = 3$, chosen arbitrarily, but with the heuristic that a large
$K$ could be hard to interpret, considering there are only 56 samples. We use
VB, because MCMC seemed to produce degenerate fits\footnote{Most of the
sampled $\theta_{ik}$ were nearly 0 or 1. Somehow, VB seems to be performing a
type of regularization.}.

\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r lda}
```

Before visualizing the results, we need to extract posterior samples for each
parameter and link them with descriptions about associated samples or microbes.
This data reshaping work is performed in the two code blocks below. The first
reshapes $\beta$, the second reshapes $\theta$.

\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r extract_beta}
```
\endgroup

\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r extract_theta}
```
\endgroup

#### Diagnostics ####

#### Visualization and Interpretation ####

The fitted parameter values are summarized in Figures \@ref(fig:visualize_beta),
\@ref(fig:visualize_theta_heatmap) and \@ref(fig:visualize_theta_boxplot). We
describe how to read each display and briefly interpret the underlying
biological.

A view of the posterior means for the $\theta_{ik}$ is given in Figure
\@ref(fig:visualize_theta_heatmap). Here, the $k^{th}$ row corresponds to the
$k^{th}$ topic, and the $i^{th}$ column gives the $i^{th}$ timepoint. That is,
the $ki^{th}$ cell is shaded in according to the posterior mean for
$\theta_{ik}$, and the sum for any column is 1.

\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r visualize_lda_theta_heatmap}
```
\endgroup

\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r visualize_lda_theta_heatmap_figure, fig.cap = "Heatmap of the posterior means of LDA estimated mixture component values over time."}
include_graphics("figure/visualize_lda_theta_heatmap-1.png")
```
\endgroup

A disadvantage of Figure \@ref(fig:visualize_lda_theta_heatmap) is that it displays
posterior means without any sense of the variability of these estimates. An
alternative which does describe uncertainty is given in Figure
\@ref(fig:visualize_thtea_boxplot). The rows and columns of panels again
represent topics and time, but now each cell includes a boxplot of posterior
samples from $\theta_{ik}$.

Both views draw attention to the two antibiotic time courses, at which point the
mixture weights place most mass on Topic 3. Topic 2 becomes depressed during the
time courses, suggesting that it summarizes the "ordinary" microbial community
structure. Topic 1 seems relatively stable over time, though it increases
somewhat in the second time course -- it might represent microbes that become
prominent in the second time course, but not the first.

Finally, even after the second peak is over, the relative contribution of Topic
3 at the end of the sampling period is somewhat larger than it had been at the
start. This perhaps reflects subtle longer term effects of antibiotics on
microbial communities. Across all topics, we find that uncertainty is typically
larger for parameters with larger values.

\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r visualize_theta_boxplot}
```
\endgroup

\blandscape
\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r visualize_theta_boxplot_figure, fig.width = 10, fig.height = 8, fig.cap = "Boxplots of approximate posteriors for estimated mixture memberships, and their evolution over time."}
include_graphics("figure/visualize_theta_boxplot-1.png")
```
\endgroup
\elandscape

To interpret these topics in terms of their microbial composition, we study
the estimated topic distributions $\beta_{\cdot k}$, displayed in Figure
\@ref(fig:visualize_beta). The three major rows correspond to the $K = 3$
estimated topics. Each boxplot within a row is associated with posterior samples
for a single microbe. Different colors identify different taxonomic families,
and within these families, microbes are sorted according to evolutionary
relatedness. Note that we have not displayed outliers, because they make the figure difficult to read.


\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r visualize_beta}
```
\endgroup

\blandscape
\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r visualize_beta_figure, fig.width = 10, fig.height = 6, fig.cap = "The distribution over microbes associated with each LDA topic."}
include_graphics("figure/visualize_beta_boxplot-1.png")
```
\endgroup
\elandscape

In light of the mixture probabilities in Figure
\@ref(fig:visualize_theta_boxplot), those microbes with large probabilities in
the third row in Figure \@ref(fig:visualize_beta) make up a large
fraction of the samples taken during antibiotic time courses. This could be due to these microbes thriving during the
regimen or other microbes being more negatively impacted -- viewing the raw data
directly supports the latter scenario. The fact that these distribution is
relatively more concentrated on a small subset of microbes with high
probabilities reflects a decrease in community diversity during antibiotic time
courses. It is interesting that the two time courses can be identified with
different microbial signatures.

We also note "bumps" of neighboring microbes with similarly elevated topic
probabilities. It is encouraging that, even without specifying smoothness along
the tree in the prior for the $\beta_{\cdot k}$s, such smoothness arises in the
fitted model.

### Unigram Model ###

Next, we apply the unigram model to the same data. While we might generally
suspect that gut microbiome profiles can change slightly over several days and
that there might be an immediate antibiotic effect, we still choose to avoid
explicitly setting $\sigma$. Instead, we use a hyperprior, $\sigma^{2} \sim \InvGam
\left(\frac{1}{2}, \frac{1}{2}\right)$, which is flat on a log-scale. Further,
since visualizing $\tilde 50$ $\beta_{t}$ is difficult, we group timepoints into
windows of length four days, so that there are fewer $\beta_{t}$'s to display.
A reasonable alternative would have been to display every fourth estimated
$\beta_{t}$ (we would expect a more slowly evolving estimated $\sigma$).

The code chunks below prime our environment for the subsequent analysis.

\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r unigram_setup}
rm(list = ls())
read_chunk("../src/antibiotics-study/unigram.R")
```
\endgroup

\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r setup}
```
\endgroup

Next, we load and filter to the same samples and microbes considered in the LDA
example of Section \@ref(sec:antibiotics-lda).

\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r get-data}
```
\endgroup

As before, we round the transformed data in order to accomodate a count-based
probabilistic model. We further define the windows that pool neighboring
timepoints to the same $\beta_{t}$s.

\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r vis-times}
```
\endgroup

The block below estimates the unigram model using VB.

\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r run-model}
```
\endgroup

The only parameters to inspect for the unigram model are the sequence
of $\beta_{t}$. To interpret our estimates in terms of the taxonomic context
associated with each microbe and sampling context associated with each time
point, we merge our parameter estimates with external information. As before, we
impute missing taxonomic family labels with those available at the next higher
(order) level and ensure microbes are ordered according to evolutionary
proximity. These postprocessing steps are performed in the code block below.

\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r prepare-beta}
```
\endgroup

Our first view of the $\beta_{t}$, given in Figure \@ref(fig:unigramseries),
tracks the time series of posterior means, grouped by taxonomic family. Here,
each line represents the sequence $\left(\beta_{tk}\right)_{t}$ for a single
microbe $k$. Each color / panel is associated with a taxonomic family, sorted
from most to least prevalent. Note that the $\beta_{tk}$ have been globally
centered -- this makes no difference in the associated probabilities.

\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r unigramseries}
```
\endgroup

\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r unigramseries_figure, fig.caption = "The evolution of per-microbe posterior means over time. The first and second antibiotics time courses are introduced at days 12 and 44, respectively."}
include_graphics("figure/unigramseries-1.png")
```
\endgroup

While useful for building general intution about the fitted model, Figure
\@ref(fig:unigramseries) is unsatisfactory for two reasons. First, it only
displays point estimates -- the posterior mean -- for each $\beta_{kv}$, which
leaves uncertainty in the estimate unknown. Second, in families with many
microbes, many of the paths occlude one another. Figure
\@ref(fig:unigramboxplots) is the time series version of Figure
\@ref(fig:visualize_beta), and it is designed to overcome these limitations.

\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r unigramboxplots}
```
\endgroup

\blandscape
\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r unigramboxplots_figure, fig.width = 10.5, fig.height = 8, echo = FALSE, fig.caption = "The evolution of per-microbe posterior quantiles over time, as different antibiotics time courses are introduced."}
```
\endgroup
\elandscape

In Figure \@ref(fig:unigramboxplots), each boxplot represent the posterior
quantiles of $\beta_{kv}$ for a single microbe. Each row is associated with a
set of timepoints, according to the windowing strategy described in the data
preparation phase. The labels on the far right describe the sampling condition
at that timepoint -- e.g., 1st Cp (Cipriofloxacin) indicates samples associated
with the first time course of antibiotics. Individual microbes are sorted
according to evolutionary relatedness, and colors / columns indicate different
taxonomic families.

We note as an aside that, while it is possible to plot the raw
probabilities\footnote{These are in fact a column of the \texttt{beta\_hat}
data.frame in the code chunks above.} $p_{tk} =
\frac{\exp{\beta_{tk}}}{\sum_{k^{\prime}} \exp{\beta_{tk^{\prime}}}}$, we find
the $\beta_{tk}$s to be on a more useful scale, allowing comparison of slightly
different small probabilities. This observation is in line with the discussion
in Figure 23.8 of [@mackay2003information].

### DTM ###

We can attempt blend mixture and temporal evolution interpretations of the
antibiotic data using the DTM. Now, the complexity of choosing the topic
evolution rate $\sigma$ in the dynamic unigram model is compounded by the
addition of a new mixture evolution rate parameter, $\delta$. Moreover, these
parameters are tied to eachother in the posterior -- it is easier to imagine
mixture memberships remaining steady over time if topics are allowed to evolve
rapidly. In absence of any more definitive domain knowledge, we choose to place
identical $\InvGam\left(\frac{1}{2}, \frac{1}{2}\right)$ priors on both
$\sigma^{2}$ and $\delta^{2}$.

By this point, the general arc of these modeling experiments should be familiar:
we first clear our workspace, reload and prepare the data, fit the model of
interest, and extract and comment on the output. This encapsulates the usual
inferential stage of data modeling, where a particular model is posited and
estimated, but omits the important stage of model revision and comparison, which
we defer at this point.

We first refresh our workspace and prepare the data to model.
\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r dtm_setup}
rm(list = ls())
read_chunk("../src/antibiotics-study/dtm.R")
```
\endgroup

\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r setup}
```
\endgroup

\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r get-data}
```
\endgroup

\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r vis-times}
```
\endgroup

\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r run-model}
```
\endgroup

\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r prepare-alpha}
```
\endgroup

\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r prepare-mu}
```
\endgroup

\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r visualize-alpha}
```
\endgroup

\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r visualize-mu}
```
\endgroup

\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r visualize-alpha-figure, fig.width = 6}
include_graphics("figure/visualize-alpha-figure-1.png")
```
\endgroup

\blandscape
\begingroup
\setstretch{1}
\fontsize{8pt}{8pt}
```{r visualizemufigure, fig.width = 10, fig.height = 6, echo = FALSE}
include_graphics("figure/visualize-mu-figure-1.png")
```
\endgroup
\elandscape

## References ##
