---
documentclass: report
title: "Evaluating Unsupervised Probabilistic Models of the Microbiome"
date: "`r Sys.Date()`"
keep-tex: true
output:
  bookdown::pdf_book:
    keep_tex: yes
    includes:
      in_header: preamble.tex
bibliography: ["unsupervised_prob.bib"]
link-citations: yes
---

# Context #

Generally, microbiome studies attempt to characterize variation in microbial
abundance profiles across different experimental conditions
[@human2012structure] For example, it can be scientifically meaningful to
describe differences in microbial communities between treatment and control
groups. Alternatively, a study may attempt to describe changes in microbial
abundances after deliberately induced perturbations
[@dethlefsen2011incomplete].

In the process, there tend to be two complementary difficulties. First, the data
are often high dimensional, measured over several hundreds or thousands of
microbes. Studying patterns at the level of individual microbes is typically
infeasible. Second, it can be important to study microbial abundances in context
of known biological information. For example, if a collection of evolutionarily
related microbes all change in sync with each other, the scientific conclusion
would be different compared to if those microbes were not related to one
another. Further, metagenomic or proteomic data can often supplement raw
microbial abundance data, providing functional context.

## Data Description ##

In this study, we focus on 16S sequencing data, one of the original approaches
to quantifying microbial abundances, and still one of the most common protocols
for describing microbial ecology. Briefly, this procedure relies on the fact
that the 16S gene is conserved across most bacterial species, but is variable
enough to differ across different strains. Hence, by counting number of
occurrences of different versions of this gene, it is possible to measure the
numbers of different species present in a sample, without attempting the
difficult problem of assembling genomes from the mixtures present in complex
environmental samples.

After preprocessing, the resulting data are a samples by species counts matrix;
this is the fundamental object for exploration and inference. Some frequently
encountered characteristics of these type of data are worth noting. First, the
data are often very heavy tailed, with samples dominated by a relatively small
number of very abundant microbes. To address this, it is common to transform the
data, using variance stabilizing transformations or replacing with ranks, for
example, or dividing analysis between a "core" central component and a
peripheral collection of less abundant species. Second, these data are typically
zero-inflated, which is often accounted for by filtering\footnote{Using variance or
k-over-A filtering, for example.}. Third, at the sample level, there is
typically variation in sequencing depth. Hence, there is often better precision
in the estimates for some samples than for others. This difference is often
explicitly modeled [@mcmurdie2014waste; @love2014differential; @ren2016bayesian]

## Statistical Background ##

We now review a few of the statistical modeling techniques that we believe can
be useful building blocks when performing microbiome analysis. Many of these
techniques have been borrowed from text analysis, thinking of the samples by
microbes matrix as a biological analog of the usual document-term matrix. The
idea of transferring these techniques to the microbiome is not new, but the
methods have still yet to be widely adopted [@shafiei2015biomico;
@chen2012estimating; @holmes2012dirichlet;  @chen2013variable].

We include this review because we hope for this text to be self-contained.
Further, we expect these methods to only be the starting point for more
sophisticated analysis -- in fact, the extensibility and modularity of these
methods are some of their most attractive features.

### Latent Dirichlet Allocation ###

Latent Dirichlet Allocation (LDA) is a generalization of multinomial mixture
modeling applicable to count matrices. We adopt the usual topic modeling
terminology, where each document is summarized by a vector of term counts.
Suppose there are $D$ documents across $V$ terms, and that these documents are
assumed mixtures of $K$ underlying topics, where a topic is defined as a
distribution over words. Then, LDA supposes the generative mechanism,
\begin{itemize}
\item For each word in the current document, draw a topic associated with that
word, with proportions determined by the document's fixed topic-mixture
probabilities.
\item Given the topics for each word in the document, draw a term associated
with it, according to the distribution for the topic to which that word is
assigned.
\end{itemize}

The specification is completed by defining priors for the document-topic and
topic-term distributions. We can formalize this process as follows. Let
$\theta_{d} \in \simplex^{K}$ represent the $d^{th}$ topic's mixture over the
$K$ underlying topics. Represent the term in the $n_{th}$ word of this document
by $w_{dn}$, and the associated topic by $z_{dn}$. Suppose the $k^{th}$ topic
places weight $\beta_{vk}$ on the $v^{th}$ term, so that $\beta_{\cdot k} \in
\simplex^{V}$. Then, the generative mechanism is

\begin{align*}
w_{dn} \vert \left(\beta_{\cdot k}\right)_{k = 1}^{K}, z_{dn} &\sim \Cat\left(\beta_{\cdot z_{dn}}\right) \\
z_{dn} \vert \theta_{d} &\sim \Cat\left(\theta_{d}\right) \\
\theta_{d} &\sim \Dir\left(\alpha\right) \\
\beta_{\cdot k} &\sim \Dir\left(\gamma\right).
\end{align*}
The interpretation is that each word in the document is assigned a topic
$z_{dn}$ according to the mixture weights $\theta_{i}$, and the actual terms are
only drawn afterwards, using the topic distributions $\beta_{\cdot k}$. In a standard
multinomial mixture model, each document can only belong to one topic, so there
would only be one value for $z_{dn}$ across all words.

In the microbiome application, we will find a formulation that marginalizes over
the $z_{dn}$ more convenient. Indeed, a strict analogy between text modeling and
microbiome analysis would consider each living microbe a word $w_{dn}$, while
what we are more interested in are counts of species across samples. That is,
we care more about $n_{dv} = \sum_{n = 1}^{N_{d}} \indic{w_{dn} = v}$ for each
sample $d$ and microbe $v$. Now, $n_{d\cdot} = \begin{pmatrix} n_{d1} \\ \vdots \\
n_{dV} \end{pmatrix} \in \naturals^{V}$ is a sum of $\Cat\left(\beta \theta_{d}
\right)$ variables, because $\Parg{w_{dn} = v} = \sum_{k = 1}^{K} \Parg{w_{dn} =
v \vert z_{dn} = k}\Parg{z_{dn} = k} = \beta_{v\cdot}^{T}\theta_{d}$. Hence, we
can write the marginal distribution as

\begin{align*}
n_{d\cdot} \vert \left(\beta_{k}\right)_{1}^{K} &\sim \Mult\left(n_{d\ast}, \beta \theta_{d}\right) \\
\beta_{\cdot k} &\sim \Dir\left(\gamma\right), k = 1, \dots, K \\
\theta_{d} &\sim \Dir\left(\alpha\right), d = 1, \dots D
\end{align*}
where $n_{d\ast} := \sum_{v = 1}^{V} n_{dv}$. Interpreting this model, we
imagine we think of each document $d$ as a mixture $\theta_{d}$ across $K$
topics.

The underlying model can be interpreted geometrically by identifying each topic
$\beta_{\cdot k}$ with a point on the $V$-dimensional simplex. Then the
expectation of the distribution over terms for the $d^{th}$ document, $\beta
\theta_{d}$ is a new point between the different $\beta_{\cdot k}$, with more
attraction towards those $\beta_{\cdot k}$ where $\theta_{dk}$ are large. See
Figure \ref{fig:simplex_lda}. That is, a collection of samples are modeled as
points lying anywhere between the $K$ latent topics. This is in contrast to
multinomial mixture modeling, which identifies each sample with one of the $K$
points $\beta_{\cdot k}$, but similar in spirit to Correspondence Analysis,
which also tries to focus interest on subsets of $\simplex^{V}$
[@greenacre1987geometric].

\begin{figure}
\centering
\includegraphics[width=7.5cm]{figure/lda_simplex_view}
\caption{The simplex view of Latent Dirichlet Allocation. Figure from [@blei2003latent].}
\label{fig:simplex_lda}
\end{figure}

## Dynamic Unigrams ##

LDA can be adapted to incorporate more specific structure. In light of the
longitudinal nature of many modern microbiome studies, one of the most
intriguing of these variants is called the Dynamic Topic Model
[@blei2006dynamic; @wang2012continuous]. This is discussed in section
\@ref(sec:dtm) -- here, we describe a related, simpler model, called the Dynamic
Unigram model, which was also investigated in [@blei2006dynamic].

The basic idea in the unigram model is to adapt the geometric view of LDA to the
time-series setting. In more detail, while LDA imagines samples being mixtures
of fundamental topics on the $V$-dimensional simplex, the Dynamic Unigrams Model
identifies a sequence of samples over time with a curve along this simplex. That
is, suppose $n_{d \cdot}$ is a document at time $t\left(d\right)$. Then, we
imagine $n_{d \cdot} \sim \Mult\left(\beta_{t\left(d\right)}\right)$, where $\beta_{t}$
are topic probabilities smoothly evolving along the simplex\footnote{We are
overloading notation in writing $t$ both as a time index for the underlying
probabilities $\beta_{t}$ and as a mapping from a document $d$ to the time that it
was observed, $t\left(d\right)$}.

A complication arises that it is not obvious how to define a prior on $\beta_{t}$
so that they trace out a curve on the simplex -- it is no longer enough to set
$\beta_{t} \sim \Dir\left(\alpha\right)$. An alternative is to link a gaussian
random walk to the simplex using a logit-normal prior. That is, define

\begin{align*}
n_{d\cdot} \vert \mu_{t\left(d\right)}  &\sim \Mult\left(\sum_{v} n_{dv}, S\left(\mu_{t\left(d\right)}\right)\right) \\
\mu_{t} \vert \mu_{t - 1} &\sim \Gsn\left(\mu_{t - 1}, \sigma^{2}I_{V}\right) \\
\mu_{0} &\sim \Gsn\left(0, \sigma^{2}I_{v}\right).
\end{align*}
where $S$ is the multilogit link
\begin{align*}
\left[S\left(\mu\right)\right]_{v} = \frac{\exp{mu_{v}}}{\sum_{v^{\prime}} \exp{\mu_{v^{\prime}}}}.
\end{align*}

This is just a reparameterization, allowing us to work with
$S\left(\mu_{t}\right)$ instead of directly with $\beta_{t}$. Note that the only
tuning parameter here is $\sigma^{2}$, which governs the smoothness of
transitions in term distributions over time. The estimated $\hat{\beta}_{tv}$ can
be thought of as a smoothing of the raw proportions $\hat{p}_{tv}:=
\frac{\sum_{d : t\left(d\right) = t}n_{dv}}{\sum_{d : t\left(d\right) =
t}\sum_{v^{\prime}} n_{dv^{\prime}}}$. That is, while LDA can be thought of a
modification of latent factor modeling to counts, the dynamic unigrams model can
be thought of as the corresponding variant of nonparametric smoothing.

## Dynamic Topic Models {#sec:dtm}

Unlike LDA, there is no notion of underlying topics in the Dynamic Unigrams
Model, but blending the two modeling ideas leads straightforwardsly to the
Dynamic Topic Model [@blei2006dynamic, @wang2012continuous]. Here, like LDA,
documents are modeled as mixtures latent topics. However, both the topics and
the mixture probabilities are modeled to evolve gradually over time.
Geometrically, the points $\beta_{\cdot k}$ within which all documents lie are
thought to shift over time. Further, the document mixtures $\theta_{d}$ aren't
scattered in the simplex without regard to time. Instead, the mixing
probabilities $\theta_{d}$ are also thought to exhibit some inertia, being more
similar among documents closer to each other in time.

This is formulated more precisely as 
\begin{align*}
n_{d\cdot} \vert \mu_{t\left(d\right)}, \alpha_{\left(d\right)} &\sim \Mult\left(\sum_{v} n_{dv}, \sum_{k = 1}^{K} S\left(\alpha_{t\left(d\right)}\right) S\left(\mu_{t\left(d\right)}\right)\right)
\end{align*}

Here, we identify $S\left(\mu_{tk}\right)$ and $S\left(\alpha_{t}\right)$ with
$\beta_{tk}$ and $\theta_{t}$, respectively.

## Nonnegative Matrix Factorization ##

Consider the samples by microbe matrix, $N \in \reals^{D \times V}$ made from
counts $n_{dv}$. In LDA, these counts are modeled by sampling from a
multinomials with total counts coming from the total number of words in each
document and probabilities coming from the rows of $\Theta B^{T}$ where $\Theta
= \begin{pmatrix}\theta_{1} \\ \vdots \\ \theta_{D} \end{pmatrix} \in
\left(\simplex^{K - 1}\right)_{\downarrow \times D}$ and $B = \begin{pmatrix}
\beta_{\cdot 1} \dots \beta_{\cdot K} \end{pmatrix} \in \left(\simplex^{V -
1}\right)_{\rightarrow \times K}$ are $D \times K$ and $V \times K$ matrices
representing document and topic distributions, respectively.

This has the flavor of factor analysis with $K$ factors, but endowed with
specific multinomial structure. To generalize without devolving into ordinary
factor analysis, which doesn't respect any of the count structure in the data,
it is natural to consider nonnegative matrix factorization, which approximates
the nonnegative matrix $N$ by the product of low rank matrices, $N \approx
\Theta B^{T}$, where now the only constraints on $\Theta$ and $B$ are that
$\Theta \in \reals_{+}^{D \times K}$ and $B \in \reals_{+}^{V \times K}$. This
is the starting point for a variety of algorithms in the Nonnegative Matrix
Factorization (NMF) literature [@wang2013nonnegative; @berry2007algorithms,
@lee2001algorithms].

Here, we will consider a Gamma-Poisson factorization model (GaP)
[@kucukelbir2015automatic; @canny2004gap], which is proposes the hierarchical
model
\begin{align*}
N &\sim \Poi\left(\Theta B^{T}\right) \\
\Theta &\sim \Gam\left(a_{0} 1_{D \times K}, b_{0} 1_{D \times K}\right) \\
B &\sim \Gam\left(c_{0} 1_{V \times K}, d_{0} 1_{V \times K} \right),
\end{align*}
where mean that each entry in these matrices is sampled independently, with
parameters given by the corresponding entry in the parameter matrix.

Unlike LDA, GaP models the total number of appearances of each topic implicitly
via the gamma prior on $\Theta$. When the shape parameter is small, this prior
is quite skewed -- this reflects the suspicion that most samples will be
dominated by only a few of the $\beta_{\cdot k}$. Further, when the shape
parameter is large, the gamma converges to a normal distribution, and so the GaP
model then reduces to ordinary factor analysis.

We can modify this model to reflect zero-inflation.

# Simulation Studies #
